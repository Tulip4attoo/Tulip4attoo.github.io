<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>生命不止，折腾不息</title>
    <description>生命不止，折腾不息</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Tue, 01 Apr 2025 02:31:15 +0700</pubDate>
    <lastBuildDate>Tue, 01 Apr 2025 02:31:15 +0700</lastBuildDate>
    <generator>Jekyll v4.2.2</generator>
    
      <item>
        <title>Alexander’s Unusual Resilience</title>
        <description>&lt;p&gt;Most conquerors who win rapidly and easily early in their careers inevitably crash hard when genuine failure finally arrives. Early victories generate confidence, but they also frequently breed complacency, stubborn arrogance, and rigid thinking. Conquerors who only taste success rarely develop humility, adaptability, and resilience—qualities critical for real long-term psychological strength.&lt;/p&gt;

&lt;p&gt;Xiang Yu, the celebrated Chinese general, illustrates this pattern vividly. Rising swiftly after the fall of the Qin dynasty, Xiang Yu secured striking victory after victory, apparently effortlessly. Yet these easy early triumphs blinded him psychologically: His confidence became arrogance; his boldness became reckless stubbornness. Xiang Yu refused advice, scorned strategy, and treated rivals and allies alike with curt disdain, convinced of his invincible fate. Eventually faced with Liu Bang—a foe more patient, careful, and psychologically wise—Xiang Yu suffered his first critical defeat. Rather than regrouping thoughtfully or rationally, Xiang Yu immediately spiraled out of emotional control. His shattered pride prevented him from rationally adapting to the changed reality. Unable to endure humiliation or learn indirectly from failure, Xiang Yu unraveled emotionally and psychologically, ultimately killing himself rather than enduring his moment of shame. Once he fell, he could never rise again.&lt;/p&gt;

&lt;p&gt;Napoleon, centuries later, similarly rose with shocking speed and ease. His rapid conquest and effortless domination of Europe led him to believe himself virtually invulnerable. Early easy victories convinced Napoleon of his own supreme genius and fate-driven destiny, tempting him to reckless ambition and stubborn inflexibility. When Napoleon led his vast army into Russia without proper logistical planning or humility—ignoring countless critical warnings and refusing to reconsider or adapt—he finally suffered an irreversible, catastrophic defeat. For the first time ever, Napoleon’s brilliant facade cracked completely, revealing hidden depths of psychological fragility. Instead of adapting and regrouping thoughtfully, Napoleon stubbornly refused to acknowledge errors, recklessly doubling down on impulse, arrogance, and ego-driven decisions. Within a short time, Napoleon’s emotional clarity and self-control collapsed dramatically and permanently. Spiraling swiftly from defeat to reckless overextension to exile and humiliation, Napoleon never again recovered his psychological greatness or regained his previous heights of power.&lt;/p&gt;

&lt;p&gt;Why is it that early success tends so consistently toward later psychological disaster? Is resilience only achievable through explicit, painful defeat?&lt;/p&gt;

&lt;p&gt;Liu Bang, who defeated Xiang Yu, began much differently. Early humiliations taught Liu Bang patience, humility, subtlety, loyalty, and strategic adaptability—he essentially gained psychological armor precisely because his early life was filled with failures rather than victories. Temujin, later the legendary Genghis Khan, endured even worse humiliation as a youth, hunted, betrayed, nearly starved, severely tested by ruthless foes. Yet these agonies, rather than breaking him, gave Temujin immense mental toughness. Early suffering thus forged both Liu Bang and Genghis Khan into deeply wise conquerors, enduring and psychologically mature enough to survive late-stage setbacks.&lt;/p&gt;

&lt;p&gt;Alexander the Great stands apart precisely because he doesn’t fit either pattern neatly. He conquered earlier, faster, and more decisively than almost anyone in history. Yet he never collapsed psychologically as Xiang Yu or Napoleon did. He never suffered the early painful lessons Liu Bang or Temujin endured, yet he developed equal or possibly greater psychological resilience. Unlike these men, Alexander somehow never tasted catastrophic defeat, humiliation, or disaster that normally teaches valuable emotional lessons—yet he remained astonishingly resistant to emotional collapse.&lt;/p&gt;

&lt;p&gt;Still, Alexander remained human, subject inevitably to emotional frailties common to all mortals. He was neither perfectly immune to frustration nor above fleeting moments of anger or despair. Nevertheless, genuine psychological greatness is defined not by the absence of emotion but rather by one’s actions—particularly how one acts during emotionally charged, difficult situations. A prime example of Alexander’s deep emotional discipline, humility, and wisdom appeared at the Hyphasis (Beas River). Standing at the threshold of pushing his conquests further east, after more than ten continuous years of unprecedented victories—having already conquered virtually every land known to the Greeks—Alexander’s weary soldiers refused to march deeper into unknown territory, beyond where any army had ever ventured. Almost any other conquering leader who, like Alexander, had never yet tasted genuine defeat would surely have struggled to accept such opposition calmly. Pride, arrogance, and momentum typically compel conquerors forward stubbornly and blindly. Yet Alexander paused, carefully listened to the emotional and physical exhaustion expressed by his men, and chose wisely to halt the advance, respecting their pleas. This remarkable choice showcases Alexander’s profound emotional resilience, careful self-control, and strategic humility—even under immense pressure and temptation. Far from diminishing him, this disciplined decision at Hyphasis truly confirms and enhances Alexander’s psychological greatness, showcasing precisely his subtle mastery in managing human emotional vulnerability.&lt;/p&gt;

&lt;p&gt;One explanation clearly stands out if you observe Alexander carefully: he mastered the rare art of indirect learning from history and literature. Alexander famously carried Homer’s Iliad everywhere—even placing it beneath his pillow at night. He deeply admired Achilles, almost obsessively. But Alexander wasn’t naively copying Achilles. He actively sought strategies to correct Achilles’s fatal impulsiveness and rage. He wanted Achilles’s bravery, charisma, ambition—but carefully learned to avoid Achilles’s tragic psychological errors. Where Achilles famously fell due to emotional blindness, Alexander worked consciously to keep his clarity, calm, and self-control.&lt;/p&gt;

&lt;p&gt;Alexander also carefully studied Herodotus’s vivid histories, which taught repeatedly how hubris destroyed mighty kings and conquerors. Xerxes invaded Greece recklessly and lost catastrophically; Cambyses wrecked himself impulsively in Egypt. Croesus, blinded by false pride in wealth, invited tragic endings. Instead of merely casually reading, Alexander genuinely internalized these vivid tragedies. By deeply imagining their emotional pain and disaster—yet without personally experiencing it—Alexander indirectly acquired psychological resilience against hubris. He became cautious, adaptable, subtly wise without ever personally tasting the bitterness of defeat himself.&lt;/p&gt;

&lt;p&gt;Alexander also enjoyed privileged early education under Aristotle, studying philosophy, politics, logic, and human emotional behavior. Aristotle notoriously emphasized rational emotional control and humility. Alexander learned from Aristotle how to step back from pride, temper, and impulse. Aristotle gave Alexander high-quality psychological armor—precise tools of rational analysis about human behavior, emotional maturity, and strategic clarity under stress. Alexander clearly employed Aristotle’s teachings each time his patience or restraint was tested, helping him remain psychologically undefeated.&lt;/p&gt;

&lt;p&gt;Yet one final critical factor makes Alexander unique, beyond even history and philosophy: Alexander sincerely believed himself semi-divine. Famously he journeyed hundreds of kilometers in harsh desert conditions to Egypt’s Siwa Oasis solely to ask priest-oracles, “Am I truly Zeus’s son?” When their answer was affirmative, Alexander genuinely believed his destiny had divine sanction. Afterward he consciously behaved as godlike as possible, aiming to transcend normal human fragilities, insecurities, and petty arrogance. He graciously honored the Persian royal family after defeating King Darius, never humiliating Darius’s captured relatives, treating them respectfully as if he indeed were divine and thus above revenge or pettiness. Alexander repeatedly and surprisingly cooperated warmly with many defeated local rulers like those in India who accepted his empire—an attitude far closer to divine magnanimity than ordinary human conquerors normally display.&lt;/p&gt;

&lt;p&gt;This sincere divine belief uniquely insulated Alexander’s mind from emotional collapse where Xiang Yu or Napoleon failed. Those conquerors invested their pride and identity exclusively in earthly victories, status, and reputation. When these failed, their minds instantly crumbled. But because Alexander considered himself divine or semi-divine—far above normal human shame—failure or humiliation were not devastating emotional blows to him. His elevated sense of self-worth helped him rationally adapt and calmly re-strategize whenever he faced resistance, setbacks, rebellions, or withdrawal. He never panicked emotionally, precisely because he psychologically lived in a world mentally-insulated by sincere divine belief.&lt;/p&gt;

&lt;p&gt;Thus Alexander’s psychological invincibility combined into a rare but coherent pattern: indirect learning through history (Herodotus), literary empathy (Homer’s Achilles), disciplined rational self-awareness (Aristotle’s philosophy), and genuinely elevated self-conception (a divine, semi-godlike status). These powerful factors prevented Alexander from falling into the normal psychological traps for early-success conquerors. Unlike Temujin or Liu Bang, he never needed painful personal lessons to achieve wisdom. Unlike Xiang Yu or Napoleon, he never mentally collapsed despite extreme pressures. Alexander carefully internalized defeat without personally tasting it—he empathetically learned tragically vivid lessons indirectly.&lt;/p&gt;

&lt;p&gt;Is failure necessary for resilience? Usually yes. For almost everyone indeed. Liu Bang and Genghis Khan became psychologically powerful precisely through humiliating early defeat and misery. And for those who never suffered early failure—like Xiang Yu—overwhelming early victories eventually proved psychologically catastrophic once defeat became inevitable. But Alexander showed there is also another path—astonishingly rare but genuinely possible: consciously and carefully learning profound strategic, emotional, and psychological lessons indirectly through history, literature, philosophy, and self-belief.&lt;/p&gt;

&lt;p&gt;That subtle path allowed Alexander to internalize humility without humiliation, caution without catastrophe, resilience without defeat. Alexander’s true greatness, then, was something more impressive and psychologically subtle than conquest itself: he uniquely demonstrated a beautiful yet forgotten truth that you can actually learn enough of defeat’s vital lessons indirectly—in literature, history, philosophy and even theology—that you never need suffer direct defeat at all.&lt;/p&gt;

</description>
        <pubDate>Mon, 31 Mar 2025 00:00:00 +0700</pubDate>
        <link>http://localhost:4000/Alexander-the-great/</link>
        <guid isPermaLink="true">http://localhost:4000/Alexander-the-great/</guid>
        
        
      </item>
    
      <item>
        <title>Text is incredible</title>
        <description>&lt;p&gt;When was the last time you seriously thought about text? Yes, plain old words—the kind you’re reading right now.&lt;/p&gt;

&lt;p&gt;We’re constantly told video is more vivid, audio more personal, and images speak louder than words. But text remains uniquely powerful. In fact, text has quietly shaped our thinking and learning habits for centuries—and it continues to do so now more than ever, even as video and audio surge around us.&lt;/p&gt;

&lt;p&gt;Why?&lt;/p&gt;

&lt;p&gt;To understand this, let’s dissect clearly how text works, and why—despite smartphones filled with streams of short-form content—it’s still the superior medium for thinking deeply.&lt;/p&gt;

&lt;h1 id=&quot;text-is-unique-as-a-reader&quot;&gt;Text is Unique (As a Reader)&lt;/h1&gt;

&lt;h2 id=&quot;pause-and-think&quot;&gt;Pause-and-Think&lt;/h2&gt;

&lt;p&gt;Real knowledge isn’t something you passively absorb. Knowledge sinks deeper only when you stop and reflect—when you connect ideas, challenge assumptions, and argue internally.&lt;/p&gt;

&lt;p&gt;Pause-and-think isn’t just a nice feature of learning—it is learning. Without pauses for reflection, information slips briefly across the surface of your attention and disappears. You might recognize the facts you’ve seen, but you’ll rarely internalize or deeply understand them. Reflection turns mere information into lasting wisdom and insight.&lt;/p&gt;

&lt;p&gt;Text uniquely encourages reflection. Because it’s static, words always wait patiently for your mind to catch up. A video or podcast flows relentlessly onward, but text makes reflection natural and inevitable. Text asks quietly but persistently: Do you really understand me? What does this mean? What else connects here?&lt;/p&gt;

&lt;p&gt;In other words, reflection is the crucial ingredient in converting facts into knowledge and insights into wisdom. Text, more than any other medium, explicitly forces and invites this vital habit of pause-and-think.&lt;/p&gt;

&lt;h1 id=&quot;controlling-your-own-pace&quot;&gt;Controlling Your Own Pace&lt;/h1&gt;

&lt;p&gt;Your mind doesn’t learn on a schedule set by someone else. Real learning requires flexibility—speeding up through straightforward ideas, slowing down, pausing, and re-reading when things get complicated.&lt;/p&gt;

&lt;p&gt;Text is unique in letting you fully control this rhythm. You can speed up, scan briefly, stop completely, reread multiple times—all effortlessly. Text is naturally accommodating: It never pushes you behind or drags you forward against your will, unlike audio or video.&lt;/p&gt;

&lt;p&gt;Yes, technically you can pause a video or a podcast. But those pauses are unnatural: they interrupt the speaker’s rhythm, distract your attention, and require explicit intention. In contrast, text’s pauses feel natural, organic, inevitable. You pause because you’re thinking—not because you’re interrupting.&lt;/p&gt;

&lt;p&gt;This reader-controlled pace seamlessly turns passive consumption into active thinking, making text uniquely effective for turning information into lasting knowledge and deep reflection.&lt;/p&gt;

&lt;h2 id=&quot;non-linear-movement&quot;&gt;Non-Linear Movement&lt;/h2&gt;

&lt;p&gt;Your mind is rarely a straight line. Why should reading be?&lt;/p&gt;

&lt;p&gt;Imagine listening to a podcast. Halfway through, you recall something from an earlier segment. Finding that exact moment in audio feels slow, tedious, even impossible. That friction significantly discourages deeper connections between ideas.&lt;/p&gt;

&lt;p&gt;Text, on the other hand, is designed for instant, effortless, non-linear jumps. Flick your eyes up one paragraph or page back, and you are instantly reconnected to past ideas. Jumping forward to see context or comparisons elsewhere is effortless. Your brain loves and needs non-linearity—connections, jumps, and backtracking—to deepen understanding. Text is the perfect match for your mind’s powerful associative, non-linear structure.&lt;/p&gt;

&lt;h1 id=&quot;text-as-a-cognitive-tool-as-a-writer&quot;&gt;Text as a Cognitive Tool (As a Writer)&lt;/h1&gt;

&lt;p&gt;If reading text is powerful and reflective, creating text—writing—is even more transformative.&lt;/p&gt;

&lt;p&gt;Most of us misunderstand writing. We think clearly first, then “record” those clear thoughts. But usually, thinking clearly happens precisely because you are writing.&lt;/p&gt;

&lt;p&gt;Writing forces clarity. If your idea is vague or contradictory, writing quickly reveals it. Writing exposes mental weaknesses mercilessly, forcing you to rethink, reorder, and restructure your ideas on the fly.&lt;/p&gt;

&lt;p&gt;Because text is flat and minimal—no voice tone, no facial cues, no background music—it demands total clarity. Ideas can’t hide behind charismatic speech, aesthetic video editing, or emotional nuance. Text removes distractions, allowing ideas to stand nakedly clear. If your thinking is weak or ambiguous, the merciless flatness of text immediately exposes it.&lt;/p&gt;

&lt;p&gt;Thus, writing is really a ruthless training method for clear thinking itself—a form of active cognitive restructuring. You’re not just transcribing ideas: writing literally rewires your ideas in real time, reshaping the structure of your thought in subtle yet profound ways.&lt;/p&gt;

&lt;h1 id=&quot;our-current-world-and-how-we-should-learn&quot;&gt;Our Current World (And How We Should Learn)&lt;/h1&gt;

&lt;p&gt;In today’s world, audio and video dominate. How should we handle learning from these formats?&lt;/p&gt;

&lt;p&gt;For long-form content—meaningful podcasts, lectures, documentaries—you can still learn well, but you must consciously practice reflection. Pause deliberately after key points. Step away often, asking yourself: What did I just learn? How does this relate to other ideas I know? Doing this regularly imitates the natural rhythm of text-based learning. It’s not as smooth as reading text, but careful habits make thoughtful learning achievable.&lt;/p&gt;

&lt;p&gt;Short-form content, however, presents a bigger challenge. Platforms are intentionally built to prevent reflection. Videos autoplay rapidly, aiming specifically to keep you running rather than stopping to think. If you’re going to use short content, change how you consume it. Watch deliberately, slowly, and if possible, off-platform—one video at a time. Avoid nonstop scrolling. Use them sparingly for relaxation or quick information breaks, never mistaking them for deep learning.&lt;/p&gt;

&lt;p&gt;Real learning, regardless of medium, always requires deliberate pauses, reflection, and thoughtful pacing. Without these, no amount of content will truly deepen your understanding or wisdom.&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;Text matters profoundly and uniquely.&lt;/p&gt;

&lt;p&gt;Our job is to consciously pick the mediums that help us think, learn, create, and generate insight most effectively—text first and foremost.&lt;/p&gt;

&lt;p&gt;With information formats, much like food, your choices have lasting consequences. You become not just what you consume, but how you consume it.&lt;/p&gt;

&lt;p&gt;So choose consciously. Choose carefully. Choose text.&lt;/p&gt;
</description>
        <pubDate>Sun, 30 Mar 2025 00:00:00 +0700</pubDate>
        <link>http://localhost:4000/Text-is-incredible/</link>
        <guid isPermaLink="true">http://localhost:4000/Text-is-incredible/</guid>
        
        
      </item>
    
      <item>
        <title>It&apos;s a Number Game</title>
        <description>&lt;h1 id=&quot;its-a-number-game&quot;&gt;It’s a Number Game&lt;/h1&gt;

&lt;p&gt;You’ve probably heard the phrase countless times: “It’s a number game.”&lt;/p&gt;

&lt;p&gt;Salespeople rely on it. Startup founders swear by it. Artists whisper it after their hundredth rejection. Yet many seem to miss why the idea is so fundamentally powerful.&lt;/p&gt;

&lt;p&gt;What’s the real meaning? The math behind it is surprisingly simple:&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Outcome = P₁ + P₂ + P₃ + … + Pₙ&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Your total outcome is the sum of probabilities from each individual attempt. Clearly, success is closely tied to making more attempts (n). If each attempt has a reasonable chance of success, increasing the number of tries directly increases your total success.&lt;/p&gt;

&lt;p&gt;But beneath this simple addition lies a subtle truth that people often overlook clearly: &lt;strong&gt;Your quality isn’t fixed—you usually get better with each attempt.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Each attempt doesn’t simply offer another shot at success. It also generates insight, knowledge, and subtle but meaningful improvements. Mathematically:&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Outcome = P(x, 1, Q₁) + P(x, 2, Q₂) + … + P(x, n, Qₙ)&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Quality (Qᵢ) is rarely static. With each new attempt, you gain small insights and knowledge. As a result, Qᵢ usually rises gradually, slowly pushing your probability upward with experience.&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;../assets/img/number-game/prob_increased.png&quot; width=&quot;100%&quot; /&gt;&lt;br /&gt;
  &lt;i&gt;Quantity could improve quality&lt;/i&gt;
&lt;/div&gt;

&lt;p&gt;This leads directly to a practical, powerful insight:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;First, reach a minimally acceptable quality level (good enough probability per attempt).&lt;/li&gt;
  &lt;li&gt;Then, rapidly increase your number of attempts (n).&lt;/li&gt;
  &lt;li&gt;Most importantly, learn and gently improve quality further with EACH attempt.&lt;/li&gt;
  &lt;li&gt;Because few people consciously combine these simple factors, applying this wisdom in real life can put you surprisingly far ahead.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;real-life-examples-clearly-explained&quot;&gt;Real-life examples clearly explained&lt;/h1&gt;

&lt;h2 id=&quot;example-1-momentum-in-startups&quot;&gt;Example 1: Momentum in Startups&lt;/h2&gt;

&lt;p&gt;Sam Altman, founder of OpenAI and former president of Y Combinator, captures this idea clearly in one sentence:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“One of the most important jobs you have as a founder is to never lose momentum.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;../assets/img/number-game/momentum.png&quot; width=&quot;100%&quot; /&gt;&lt;br /&gt;
  &lt;i&gt;Don&apos;t lose momentum&lt;/i&gt;
&lt;/div&gt;

&lt;p&gt;But what does Altman really mean by momentum, practically speaking? Momentum means continuing rapidly—making more attempts (increasing n) faster and learning fast enough to improve the quality (Qᵢ) of each next attempt subtly.&lt;/p&gt;

&lt;p&gt;Why is losing momentum dangerous? Because it’s easy to underestimate how quickly your chances decline if you pause and lose touch. Pauses reduce feedback, learning opportunities disappear, and quality quickly drops. Thus, momentum is not merely speed. It is speed plus the consistent improvement of each attempt.&lt;/p&gt;

&lt;p&gt;By maintaining rapid, repeated attempts, startup founders strategically leverage both scale (quantity) and learning (quality) together.&lt;/p&gt;

&lt;h2 id=&quot;example-2-endurance-and-uncertainty&quot;&gt;Example 2: Endurance and Uncertainty&lt;/h2&gt;

&lt;p&gt;Another insightful example clearly reveals the powerful interplay of quantity and quality:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“You beat 99% of people—not by being smarter or luckier—but by enduring pain and uncertainty longer.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In practice, most people quit after a small number of tries, especially if initial probabilities feel low. They get discouraged easily and stop prematurely.&lt;/p&gt;

&lt;p&gt;Those who instead increase their number of attempts (n) by enduring uncertainty reap two huge benefits clearly:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;A large number of attempts itself increases the chance of eventual success.&lt;/li&gt;
  &lt;li&gt;But more importantly, repeated attempts build experience and insight. With experience, their quality naturally climbs. Each subsequent attempt becomes gradually more effective.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Thus, in difficult scenarios, the ability to endure increases both quantity (attempts) and quality (learning from attempts).&lt;/p&gt;

&lt;h1 id=&quot;important-nuance-and-context&quot;&gt;Important nuance and context&lt;/h1&gt;

&lt;p&gt;Of course, the “numbers game” principle clearly depends on your context. Not all scenarios allow multiple attempts easily.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;For instance, war often only gives one opportunity: you either win decisively or lose everything. In this scenario, quantity (number of attempts) matters far less. Instead, quality—making sure your probability is extremely high—is absolutely critical.&lt;/li&gt;
  &lt;li&gt;But in business, startups, careers, selling, publishing, investing, the scenario is exactly opposite. You usually receive many attempts. Optimizing quantity and maintaining gentle improvement in quality is clearly and powerfully beneficial.&lt;/li&gt;
&lt;/ul&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;../assets/img/number-game/oneshot.png&quot; width=&quot;100%&quot; /&gt;&lt;br /&gt;
  &lt;i&gt;Sometimes, we only have 1 shot&lt;/i&gt;
&lt;/div&gt;

&lt;p&gt;The power of the idea clearly depends on your situation. Identify that clearly first, then apply the “numbers game” principle strategically.&lt;/p&gt;

&lt;h1 id=&quot;putting-the-numbers-game-to-work-clear-concise-practical-action&quot;&gt;Putting the Numbers Game to Work (clear, concise, practical action):&lt;/h1&gt;

&lt;p&gt;To harness the practical strength of the “numbers game” idea:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;First establish a “good-enough” quality threshold. Simply improve until you’re just above acceptable probability.&lt;/li&gt;
  &lt;li&gt;Immediately after, scale aggressively by multiplying your attempts. (“Never lose momentum.”)&lt;/li&gt;
  &lt;li&gt;Each attempt, consciously observe and learn. Always gently raise quality further using your new insights.&lt;/li&gt;
  &lt;li&gt;Be patient. Your growing experience gradually improves your effectiveness.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;That’s all clearly there is to it: Set a baseline quality. Maintain high scale and momentum. Learn continually along the way.&lt;/p&gt;

&lt;p&gt;This is what people really mean by “it’s a number game.” Beneath the surface of simple repetition is a beautiful, subtle, deeply practical learning loop.&lt;/p&gt;

&lt;p&gt;Easy to say. Powerful to apply. Surprisingly deep beneath the simplicity.&lt;/p&gt;
</description>
        <pubDate>Sat, 29 Mar 2025 00:00:00 +0700</pubDate>
        <link>http://localhost:4000/Number-game/</link>
        <guid isPermaLink="true">http://localhost:4000/Number-game/</guid>
        
        
      </item>
    
      <item>
        <title>Truly multimodal</title>
        <description>&lt;h1 id=&quot;i-giới-thiệu&quot;&gt;I. Giới thiệu&lt;/h1&gt;

&lt;p&gt;Năm ngoái, GPT-4o ra mắt, với “o” là “omni”, ám chỉ việc đây là một model multimodal đích thực, không chỉ còn là việc đơn giản plug với các model khác. Những ngày vừa qua, Gemini và Grok cũng theo đó cho ra những model có khả năng output trực tiếp ra hình ảnh.&lt;/p&gt;

&lt;p&gt;Ngay hôm nay, GPT-4o cuối cùng đã ra mắt image generation, với khả năng tạo ra những bức ảnh rất choáng ngợp. Dễ dàng tạo những bức ảnh vô cùng phức tạp với những prompt đơn giản, dễ dàng fix lỗi bằng mô tả. Quá nhiều thứ thay đổi trong image generation.&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;../assets/img/truly-multimodal/comic.png&quot; width=&quot;100%&quot; /&gt;&lt;br /&gt;
  &lt;i&gt;Fix lỗi ở comic. First shot.&lt;/i&gt;
&lt;/div&gt;

&lt;p&gt;OpenAI, mặc dù thường không tiết lộ chi tiết kỹ thuật cụ thể, lần này đã đưa ra một thông tin về model trong tài liệu công bố GPT-4o (“GPT-4o System Card – March 25, 2025”):&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“Unlike DALL-E, which operates as a diffusion model, 4o image generation is an autoregressive model natively embedded within ChatGPT.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Vậy, chúng ta cùng tìm hiểu cách mà GPT-4o và các model khác thực hiện điều này.&lt;/p&gt;

&lt;h1 id=&quot;ii-one-big-autoregressive-transformer&quot;&gt;II. One Big Autoregressive Transformer&lt;/h1&gt;

&lt;p&gt;Về cơ bản, mô hình autoregressive transformer hoạt động theo nguyên lý dự đoán token tiếp theo dựa trên các token đã có trước đó:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Token hoá dữ liệu&lt;/strong&gt;: Ví dụ, text có token riêng, hình ảnh có token riêng, audio có token riêng.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Transformer xử lý tuần tự các token&lt;/strong&gt;: Không phân biệt loại dữ liệu; từ đó học được ngữ cảnh rộng lớn từ sự tương quan giữa các dạng thông tin này.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Quy trình đơn giản là:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Token đầu vào → Transformer autoregressive → Token đầu ra (text/hình ảnh/audio)&lt;/li&gt;
  &lt;li&gt;Token đầu ra → Được decode trở lại dạng mong muốn (âm thanh hoặc pixel ảnh)&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;iii-một-số-đặc-điểm-chính-của-gpt-4o-mới&quot;&gt;III. Một số đặc điểm chính của GPT-4o mới&lt;/h1&gt;

&lt;p&gt;OpenAI thậm chí còn tung 1 hình ảnh về đặc điểm cơ bản của việc tích hợp output hình ảnh vào LLM&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;../assets/img/truly-multimodal/properties.png&quot; width=&quot;100%&quot; /&gt;&lt;br /&gt;
  &lt;i&gt;Các đặc điểm của GPT-4o&lt;/i&gt;
&lt;/div&gt;

&lt;h2 id=&quot;1-ưu-điểm&quot;&gt;1. Ưu điểm:&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Tạo ảnh với hiểu biết có sẵn từ LLM (world knowledge)&lt;/strong&gt;: Transformer đã học được lượng kiến thức khổng lồ trong quá trình huấn luyện, nay được khai thác để tạo ra hình ảnh giàu ngữ cảnh hơn.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Khả năng hiển thị, kết hợp text trong ảnh&lt;/strong&gt;: Dễ dàng và chính xác hơn trong việc nhúng thông tin (text) vào hình ảnh, điều rất khó đạt chất lượng cao bằng diffusion model trước đây (như DALL-E).&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Khả năng hiểu trực tiếp từ context&lt;/strong&gt;: Model có khả năng hiểu và tuân thủ instruction, thậm chí các semantic instruction từ input là ảnh khác.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Chỉ còn 1 stack&lt;/strong&gt;: Thay vì phải plug với nhiều stack (GPT + agent + DALL-E…), nay chỉ thuần túy là GPT-4o input và output.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;2-thách-thức&quot;&gt;2. Thách thức:&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Bit-rate khác nhau lớn giữa các loại hình data&lt;/strong&gt;:
    &lt;ul&gt;
      &lt;li&gt;Text: Token đặc trưng bởi vocab lớn (~50.000 tokens → 16 bit/token).&lt;/li&gt;
      &lt;li&gt;Images: Token hình ảnh thường có vocab nhỏ hơn (~8192 tokens → 13 bit/token), mỗi token encode lượng lớn thông tin thị giác, độ chi tiết bị mất đáng kể so với pixel nguyên thuỷ.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Tốn kém tính toán, chưa tối ưu (compute not adaptive)&lt;/strong&gt;: Transformer model autoregressive ban đầu không tối ưu tốt cho data dạng “nặng” (hình ảnh, audio) dẫn tới quá tải tính toán, giảm hiệu quả và tăng độ trễ khi output hình ảnh lớn hoặc độ phân giải cao. Với GPT-4o, ảnh có độ chi tiết và sắc nét rất cao, dẫn tới một lần gen ảnh có thể mất khoảng 30 giây, ảnh hưởng lớn tới UX.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;3-phương-án-openai-áp-dụng-để-giải-quyết-thách-thức&quot;&gt;3. Phương án OpenAI áp dụng để giải quyết thách thức:&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Sử dụng các phương pháp nén (compressed representations)&lt;/strong&gt;: Thay vì directly output pixel, transformer output các token nén cô đọng, hiệu quả tính toán và thông tin cao hơn.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Phối hợp giữa transformer autoregressive và powerful decoder&lt;/strong&gt;: Mô hình giải mã mạnh hơn, diffusion decoder.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;iv-tổng-kết&quot;&gt;IV. Tổng kết&lt;/h1&gt;

&lt;p&gt;GPT-4o thực sự đánh dấu bước ngoặt mới cho sự phát triển các AI model thế hệ tiếp theo. Truly multimodal mở ra nhiều khả năng mới cho UX, có thể dẫn đến các ứng dụng sáng tạo hơn, như các trợ lý cá nhân AI đa phương tiện.&lt;/p&gt;

&lt;p&gt;Tính toán lớn, tốn kém vẫn là vấn đề trong tương lai. Các công ty cần tối ưu hoá sâu hơn. Tuy vậy, với computational power tiếp tục tăng nhanh, một trợ lý ảo như Samantha không còn xa vời.&lt;/p&gt;
</description>
        <pubDate>Wed, 26 Mar 2025 00:00:00 +0700</pubDate>
        <link>http://localhost:4000/Truly-multimodal/</link>
        <guid isPermaLink="true">http://localhost:4000/Truly-multimodal/</guid>
        
        
      </item>
    
      <item>
        <title>Coding with AI - Dùng sao cho tốt?</title>
        <description>&lt;p&gt;Lập trình ra đời từ nửa sau thế kỷ 20 và đã trải qua nhiều thay đổi lớn. Ban đầu, lập trình viên dùng thẻ đục lỗ, với chi phí debug cực cao do mỗi lỗi phải sửa thủ công trên từng tấm thẻ. Sau đó, ngôn ngữ Assembly và các ngôn ngữ cấp cao như Fortran xuất hiện, máy tính nhanh hơn, giảm dần chi phí debug bằng cách thay thế mã máy phức tạp. Đến khi màn hình và giao diện người dùng (UX) ra đời, việc viết và sửa mã trở nên trực quan hơn. Các chu kỳ phát triển (iteration) cũng ngắn lại đáng kể, đi kèm với những cơ chế quản lý mới như Agile hay DevOps.&lt;/p&gt;

&lt;p&gt;Hiện nay, chúng ta chứng kiến 1 thay đổi lớn nữa: coding with AI, tiếp tục thay đổi cách chúng ta viết code và giải quyết vấn đề.&lt;/p&gt;

&lt;p&gt;Bài viết này chú trọng vào HOW: chúng ta nên sử dụng AI trong coding thế nào để đạt hiệu quả tốt hơn.&lt;/p&gt;

&lt;h1 id=&quot;1-current-use-cases-of-ai-in-coding&quot;&gt;1, Current Use Cases of AI in Coding&lt;/h1&gt;

&lt;p&gt;Theo dõi qua các kênh như X, có thể tạm chia ra cách mọi người sử dụng AI cho coding thành 2 hướng chính:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;từ idea -&amp;gt; MVP: Các công cụ như v0 và Lovable cho phép xây dựng sản phẩm khả thi tối thiểu (MVP) nhanh chóng, đặc biệt cho dự án web. Điều này giúp tạo MVP trong một cuối tuần là điều hoàn toàn khả thi, đặc biệt cho các dự án không quá phức tạp về kỹ thuật. Điều này sẽ thay đổi về cơ bản cách mọi người, đặc biệt là cá nhân làm MVP.&lt;/li&gt;
  &lt;li&gt;áp dụng cho daily development: Với lập trình viên thông thường, AI được tích hợp vào các IDE hỗ trợ việc code hàng ngày. Từ việc dùng completion và gợi ý, cho tới refactor và clean code trở nên nhanh gọn hơn, và việc tạo test dồng thời là document cũng trở thành những công việc nhẹ nhàng hơn rất nhiều.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;2-key-characteristics-of-ai-in-coding&quot;&gt;2, Key Characteristics of AI in Coding&lt;/h1&gt;

&lt;h2 id=&quot;key-features-and-considerations&quot;&gt;Key Features and Considerations&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;các công cụ rất khác biệt: không như trước đây, các IDE về cơ bản là giống nhau, các công cụ có những features rất khác biệt và khó để chuyển đổi hơn. Ngay ở trên, ta có thể thấy sự khác biệt giữa các IDE và bộ công cụ chuyên biệt cho làm web. Chưa kể tới, các công cụ có sự hỗ trọ về AI-agent, mang tới sự khác biệt về flow test/debug/implement.&lt;/li&gt;
  &lt;li&gt;Giảm chi phí viết code (code writing): chi phí cho code writing giảm đáng kể, dẫn tới tầm quan trọng của code reading tăng lên nhiều. Làm cách nào để đọc và hiểu code nhanh hơn trở thành 1 vấn đề thiết thực và mang tính quan trọng cao hơn (làm cách nào AI giúp ta được điều này?). Ngoài ra, việc tuyển dụng có thể sẽ thay đổi, giảm sự chú trọng vào code writing (leetcode), thay vào đó là AI và code reading (như cách chúng ta làm việc hàng ngày)&lt;/li&gt;
  &lt;li&gt;Con người vẫn phải giải quyết và hoàn thiện: có thể coi việc hoàn thành dự án thì AI có thể hoàn thành 70-80% cực kỳ nhanh, tuy nhiên 20% còn lại sẽ cực kỳ khoscho AI hoàn thiện, thường khi đạt tới 1 mức độ - về độ dài và độ phức tạp, AI sẽ gần như không tiến thêm nữa trong việc hoàn thành project. Khi đó, các lập trình viên nên thật sự nhúng tay vào giải quyết vấn đề.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;3-practical-patterns-and-strategies&quot;&gt;3, Practical Patterns and Strategies&lt;/h1&gt;

&lt;p&gt;Để tận dụng tốt lợi ích của AI trong coding, cá nhân tôi sử dụng 1 số pratical pattern sau:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;AI first draft: để AI đưa ra các giải pháp ban đầu. Nó có thể không tốt, nhưng giúp ta có cái nhìn ban đầu về dự án.&lt;/li&gt;
  &lt;li&gt;stay modular: đây là điều quan trọng bậc nhất. Giữ cho code của bạn có tính module cao, giúp dễ dàng thay thế hoặc cải thiện các thành phần được tạo ra bởi AI nếu cần. CÓ nhiều lý do khiến nó trở nên quan trọng:
    &lt;ul&gt;
      &lt;li&gt;AI hiện tại vẫn rất dễ rối khi thay đổi nhiều thứ. Việc giữ code theo modular giúp AI chỉ cần sửa những thành phần sai, giữ lại thành phần đúng 1 cách dễ hơn.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Restart When Needed: Nếu nhận thấy AI không thể giải quyết hiệu quả một tính năng hoặc đoạn code cụ thể, đừng ngần ngại bắt đầu lại từ đầu (trước khi làm feature). AI rất dễ bị stuck in a loop.&lt;/li&gt;
  &lt;li&gt;Each chat for each task: mỗi chat chỉ nên dùng cho 1 task. Hãy giữ cho context được focus và nhỏ gọn, bởi dù được tăng context size lên nhưng các model khi làm với context dài vẫn không quá hiệu quả. Liên tục review và feedback trong chat, tốt nhất nên xử lý xong task trước khi làm task khác.&lt;/li&gt;
  &lt;li&gt;Trust but verify: thường xuyên check và test lại code. Ngoài ra với AI, việc gen test case trở nên dễ dàng và nhanh chóng hơn rất nhiều. hú ý trao đổi và nghĩ về edge case (thật ra cũng như code bt thôi).&lt;/li&gt;
  &lt;li&gt;start small: đây là cách chúng ta dễ kiểm soát hơn, tránh việc code bị tràn lan sau này.&lt;/li&gt;
  &lt;li&gt;hiểu rõ các model: mỗi model có những đặc điểm khác biệt nhất định trong việc gen code. Việc nắm được khả năng của các model giúp ta có thể chọn model phù hợp hơn (nhưng cũng tùy tool, vd dùng cursor thì thường 3.7 sonnet là được rồi, nhưng vd cline sẽ khác. Sau mà dùng model để chọn LLM route thì còn khác nữa). Việc hiểu rõ model còn giúp ta biết được các feature nào có thể để code gen từ đầu tới cuối, các feature nào thì để code gen 1 đoạn và ta hướng dẫn 1 đoạn (vd theo kinh nghiệm cá nhân, LLM rất mạnh trong web và python). Tất nhiên việc “hiểu rõ” này mang tính thiên kiến cá nhân, nhưng có thể mang lại lợi ích lớn cho ta, chí ít, tốc độ sau cùng thường không chậm hơn tốc độ viết code từ đầu tới cuối, mà thường là nhanh hơn đáng kể.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;4-future-of-ai-in-coding&quot;&gt;4, Future of AI in Coding&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;Một trend được push mạnh hiện nay là AI Agent (và computer use). Rất rõ ràng, khi áp dụng 2 điều này thì việc run và test trở nên nhanh hơn và thuận tiện hơn rất nhiều. Các phản hồi cũng rõ ràng và cụ thể hơn (vì visual understanding)&lt;/li&gt;
  &lt;li&gt;Các best practice và sự thông thạo khi sử dụng AI sẽ tăng lên. Đồng thời là các model sẽ càng phản hồi tốt hơn và rẻ hơn -&amp;gt; càng dùng nhiều hơn.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;Vậy ta làm thế nào? Dùng hàng ngày, thi thoảng lên mạng check xem có cách dùng nào mới không, có tip &amp;amp; trick nào không (vd cursor rules thì có cả 1 site cung cấp cho ta), cái gì thành quy luật thì càng tốt để dùng.&lt;/p&gt;
</description>
        <pubDate>Sat, 22 Mar 2025 00:00:00 +0700</pubDate>
        <link>http://localhost:4000/Coding-with-AI/</link>
        <guid isPermaLink="true">http://localhost:4000/Coding-with-AI/</guid>
        
        
      </item>
    
      <item>
        <title>My tech recap - 2024</title>
        <description>&lt;p&gt;2024 was a failure year for me.&lt;/p&gt;

&lt;p&gt;Still continued pursuing and diving deep into genAI. Had many ideas, made some prototypes but couldn’t launch any products. Pretty terrible.&lt;/p&gt;

&lt;h1 id=&quot;what-i-learned-in-2024&quot;&gt;What I Learned in 2024&lt;/h1&gt;

&lt;h2 id=&quot;genai---llms&quot;&gt;GenAI - LLMs&lt;/h2&gt;

&lt;p&gt;I work with genAI daily through ChatGPT/Claude and cursor lol. Did a few projects at work too, though I found them quite boring.&lt;/p&gt;

&lt;p&gt;I learned some new techniques, mainly in LLM optimization. The optimization problem I encountered and thought about the most was how to improve response time with fixed hardware (without solving it by scaling up hardware). Also did a bit with Agents. Generally, I know all the LLM techniques but haven’t deployed them in real-world scale, so it’s whatever.&lt;/p&gt;

&lt;p&gt;Still believe in local LLMs, and working hard to create products for local LLMs.&lt;/p&gt;

&lt;h2 id=&quot;genai---visual-things&quot;&gt;GenAI - Visual things&lt;/h2&gt;

&lt;p&gt;I learned comfy UI, which is an awesome backend + frontend for image generation.&lt;/p&gt;

&lt;p&gt;Did a couple of projects with genAI image early in the year, learned bits and pieces.&lt;/p&gt;

&lt;h2 id=&quot;new-coding-paradigm&quot;&gt;New Coding Paradigm&lt;/h2&gt;

&lt;p&gt;Năm nay thậm chí đã dùng cursor dài hạn, 20$/tháng, mình nghĩ cũng là cái giá tạm tạm hợp lý. Mình tự tin code những thứ mà 2 3 năm trước sẽ rất khó nhằn, ví như có thể code 1 cái browser extension trong 1 ngày. Tuy vậy, vì làm việc nhiều nên mình biết nhiều cái khó khăn bên coding. Có lẽ trong tương lai 5 năm, chưa thể thay thế vị trí programmer được (vì data có vẻ là giới hạn rồi).&lt;/p&gt;

&lt;p&gt;This year I even used cursor long-term, $20/month, I think it’s a fairly reasonable price. I’m confident coding stuff that would’ve been super tough 2-3 years ago, like coding a browser extension in a day (without any knowledge of JS and extension). However, because I work a lot, I know limit of AI coding. Perhaps in the next 5 years, it still can’t replace programmers (seems like data is 1 big limitation).&lt;/p&gt;

&lt;h2 id=&quot;ui-things&quot;&gt;UI things&lt;/h2&gt;

&lt;p&gt;I learned fasthtml for making backend + frontend for web-based products. It is best in the town for this task.&lt;/p&gt;

&lt;p&gt;However, I’m currently focusing more on games/apps. One of the things I’m putting all my effort into is LLM-based games. Initially learned godot (godot 2d is really amazing), but because godot went woke, I’ll use Unity, which is also a better choice (much more support and guide, also I believe that AI know Unity better than godot).&lt;/p&gt;

&lt;p&gt;Really want to work with Love2d but now’s not the time.&lt;/p&gt;

&lt;h1 id=&quot;my-current-stackflow&quot;&gt;My current stack/flow&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;Claude for chat and research: free claude is enough, I find it smarter than chatgpt.&lt;/li&gt;
  &lt;li&gt;Cursor for coding: Cursor is much better than VScode copilot. I tried Cline but it doesn’t work well.&lt;/li&gt;
  &lt;li&gt;fasthtml for quick MVP: much better than gradio/fastapi.&lt;/li&gt;
  &lt;li&gt;Obsidian for Logs and Notes: still love it.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;what-i-plan-to-learnmake-in-2025&quot;&gt;What I Plan to Learn/Make in 2025&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;make thing&lt;/li&gt;
  &lt;li&gt;launch thing&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Last year I launched nothing. It is a failed year.&lt;/p&gt;

&lt;p&gt;Learn to build and launch is my highest priority in 2025, and no second priority. I have a killer idea that I believe it could be both technical possible and marketable, also 99% sure I could make some money from it (1 mil at least, I guess).&lt;/p&gt;

&lt;p&gt;So, do it.&lt;/p&gt;
</description>
        <pubDate>Mon, 20 Jan 2025 00:00:00 +0700</pubDate>
        <link>http://localhost:4000/My-tech-recap-2024/</link>
        <guid isPermaLink="true">http://localhost:4000/My-tech-recap-2024/</guid>
        
        
      </item>
    
      <item>
        <title>How Apple Intelligence works?</title>
        <description>&lt;p&gt;Bài viết ban đầu đăng ở đây: https://atekco.io/1718953622585-tich-phan-ve-ai-apple-intelligence/&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://atekcostorage.blob.core.windows.net/post-image/2024/6/24/1718953622585/1719198969545_apple_intelligence_(1).jpg&quot; width=&quot;100%&quot; /&gt;&lt;br /&gt;
&lt;/div&gt;

&lt;p&gt;Đúng vậy, với Apple, AI = Apple Intelligence. Khác với rất nhiều công bố trước đây, AI luôn là 1 cụm từ Apple tránh nhắc tới, nhưng trong WWDC24 gần đây, AI (Apple Intelligence) lại là cụm từ nổi bật nhất. Theo như công bố, Apple Intelligence có thể tham gia các tác vụ và trải nghiệm người dùng như viết và tinh chỉnh văn bản, ưu tiên và tóm tắt thông báo, tạo hình ảnh vui nhộn cho các cuộc trò chuyện với gia đình và bạn bè, cũng như thực hiện các hành động trong ứng dụng để đơn giản hóa các tương tác giữa các ứng dụng.&lt;/p&gt;

&lt;p&gt;Nhiều quan ngại về privacy đã nổ ra khi Apple Intelligence được công bố, đặc biệt khi Apple tuyên bố OpenAI là đối tác cung cấp một số model. Điển hình như việc Elon Musk đã tweet rằng nếu Apple tích hợp OpenAI ở mức độ hệ điều hành, ông sẽ cấm các thiết bị Apple trong các công ty của mình.&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://atekcostorage.blob.core.windows.net/post-image/2024/6/21/1718953622585/1718954267159_musk-tweet.png&quot; width=&quot;100%&quot; /&gt;&lt;br /&gt;
  &lt;i&gt;Nguồn: x.com&lt;/i&gt;
&lt;/div&gt;

&lt;p&gt;Rõ ràng Apple đã lường trước các phản ứng này. Vì vậy, Tim Cook đã nói rằng bất cứ khi nào có những thông tin cần đến OpenAI, người dùng sẽ nhận được thông báo xem có muốn thực hiện hay không, và dữ liệu sẽ chỉ được gửi đi khi có sự đồng thuận của người dùng. Do đó, ta có thể cho rằng, hầu hết các tác vụ của Apple Intelligence sẽ được thực hiện trên thiết bị hoặc trên hệ thống điện toán đám mây riêng của Apple. Nếu không, điều đó sẽ gây khó chịu về trải nghiệm người dùng.&lt;/p&gt;

&lt;p&gt;Vậy, Apple thực hiện điều này bằng cách nào?&lt;/p&gt;

&lt;p&gt;Trước hết, hãy xem xét những thách thức của on-device AI.&lt;/p&gt;

&lt;h1 id=&quot;những-thách-thức-của-on-device-ai&quot;&gt;Những thách thức của on-device AI&lt;/h1&gt;

&lt;p&gt;Có thể điểm ra những thách thức lớn nhất khi thực hiện on-device AI:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Specialization: Một trong những thách thức lớn nhất khi triển khai on-device AI là đảm bảo các mô hình AI có thể đảm bảo chất lượng của kết quả cho từng tác vụ cụ thể. Điều này đòi hỏi sự tinh chỉnh cẩn thận để đảm bảo rằng các mô hình có thể hiểu và thực hiện tốt các yêu cầu từ người dùng. Ví dụ, viết và tinh chỉnh văn bản đòi hỏi một loại mô hình AI khác với việc tạo hình ảnh vui nhộn cho các cuộc trò chuyện.&lt;/li&gt;
  &lt;li&gt;Size: Thách thức thứ hai là kích thước của các mô hình AI. Để các mô hình này có thể hoạt động hiệu quả trên các thiết bị như iPhone, chúng phải đủ nhỏ gọn. Ví dụ như iPhone 15 pro chỉ có 6GB RAM, do đó đây là một thách thức lớn. Hãy nhớ rằng mới chỉ đầu năm ngoái thôi, các mô hình tốt đều có ít nhất 7 tỷ parameters, chưa nói tới việc các model phổ biến là hàng trăm tỷ parameters. Ngoài ra, nhà phát triển cũng cần tìm cách để các mô hình chuyên môn hóa (Specialization) không tốn quá nhiều bộ nhớ - một điều vô cùng đắt đỏ đối với các sản phẩm nhà táo.&lt;/li&gt;
  &lt;li&gt;Performance: Cuối cùng, một thách thức không kém phần quan trọng là hiệu suất của các mô hình AI trong quá trình inference và tiêu thụ năng lượng. Các mô hình AI phải có khả năng cho ra kết quả nhanh chóng để cung cấp trải nghiệm người dùng mượt mà. Việc này đồng thời cũng khiến tiêu thụ năng lượng nhiều hơn, Apple phải tìm cách cân bằng giữa hiệu suất và tiêu thụ năng lượng để đảm bảo rằng người dùng có thể sử dụng các tính năng AI mà không lo lắng về việc sạc pin liên tục. Điều này đòi hỏi sự tối ưu hóa ở cả mức phần cứng và phần mềm, từ việc thiết kế chip xử lý cho đến việc tối ưu hóa thuật toán.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Vậy, Apple làm gì để đối mặt với các thách thức này? Dưới đây là những dự đoán của mình, dựa trên những thông tin mà Apple đã thông báo.&lt;/p&gt;

&lt;h1 id=&quot;coreml&quot;&gt;CoreML&lt;/h1&gt;

&lt;p&gt;CoreML là framework của Apple, giúp tích hợp các machine learning model vào những thiết bị của mình, đồng thời loại bỏ những phức tạp liên quan đến NVIDIA/CUDA. Với sự chủ động này, Apple hoàn toàn không phụ thuộc vào bất kỳ nhà cung cấp bên ngoài nào và có thể tối ưu hóa các mô hình AI theo cách riêng của mình. Apple cũng đã tích hợp sẵn nhiều tối ưu hóa trong CoreML (ví dụ như Palettization - phương pháp sẽ nhắc tới phía sau), đảm bảo rằng các biện pháp này đã được kiểm tra kỹ lưỡng và sẵn sàng sử dụng trên các thiết bị của Apple.&lt;/p&gt;

&lt;p&gt;Một điểm lợi nữa khi chạy AI trên các thiết bị của Apple là khả năng tối ưu hóa để tiêu tốn ít năng lượng hơn. Các nhà phát triển ứng dụng trên Android thường phải đối mặt với sự đa dạng của chip/GPU trên các thiết bị và thiếu sự hỗ trợ của các API tối ưu, điều này làm cho việc triển khai AI trở nên phức tạp và ít hiệu quả hơn. Nhờ CoreML, Apple không chỉ cung cấp một môi trường phát triển thuận lợi mà còn đảm bảo rằng các tác vụ AI hoạt động mượt mà và hiệu quả trên các thiết bị của mình.&lt;/p&gt;

&lt;h1 id=&quot;sử-dụng-các-foundation-model-với-kích-thước-nhỏ&quot;&gt;Sử dụng các foundation model với kích thước nhỏ&lt;/h1&gt;

&lt;p&gt;Những demo của Apple Intelligence cho thấy, nhiều khả năng các model on device của Apple chưa phải thuộc dạng multimodal, mà chia ra làm hai dạng chính là language (text) và image.&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://atekcostorage.blob.core.windows.net/post-image/2024/6/21/1718953622585/1718954264095_apple-intelligence-ecosystem.png&quot; width=&quot;100%&quot; /&gt;&lt;br /&gt;
  &lt;i&gt;Minh họa về hệ sinh thái của Apple Intelligence. Nguồn: Apple&lt;/i&gt;
&lt;/div&gt;

&lt;h1 id=&quot;text&quot;&gt;Text&lt;/h1&gt;

&lt;p&gt;Apple sử dụng một mô hình có 3 tỷ parameters (3b) để làm LLM giải quyết các tác vụ liên quan tới text. Đây là 1 model không lớn nhưng có kết quả rất tốt về Safety, Instruction-Following Eval (IFEval) và Writing.&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://atekcostorage.blob.core.windows.net/post-image/2024/6/21/1718953622585/1718954266650_model-benchmarks.png&quot; width=&quot;100%&quot; /&gt;&lt;br /&gt;
  &lt;i&gt;Model của Apple so với các model cùng kích thước. Nguồn: Apple&lt;/i&gt;
&lt;/div&gt;

&lt;p&gt;Với kích thước 3b, có thể cho rằng foundation model mà Apple dùng chính là một bản đã hiệu chỉnh của OpenELM, đây là mô hình LLM open-source của Apple mới được tung ra cuối tháng 4 vừa rồi.&lt;/p&gt;

&lt;p&gt;Ngoài ra, LLM của Apple còn sử dụng thêm một số phương pháp để optimization bằng cách sử dụng grouped-query attention (tương tự như OpenELM) để tăng tốc độ text generation của model.&lt;/p&gt;

&lt;p&gt;Apple cũng sử dụng chung một bảng vocab embedding cho input và output, từ đó có thể giảm được một phần yêu cầu về bộ nhớ cần sử dụng. Ở các thiết bị, model dùng 49k token vocab size (một điểm khác so với chỉ 32k size của OpenELM-3B-Instruct). Trên private server, model sử dụng có vocab size lớn hơn là 100k.&lt;/p&gt;

&lt;p&gt;Apple cũng có dùng context pruning. Đây là một phương pháp giảm chi phí computation của LLM bằng việc bỏ bớt một số token không mang nhiều ý nghĩa khi thực hiện text generation.&lt;/p&gt;

&lt;h1 id=&quot;image&quot;&gt;Image&lt;/h1&gt;

&lt;p&gt;Apple không đưa ra nhiều thông tin về model mà họ sử dụng, nhưng chúng ta có thể cho rằng model mà Apple sử dụng vẫn dựa trên:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Text-encoder: text encoder là 1 thành phần rất quan trọng trong việc giúp những bức ảnh đầu ra có thể tuân theo đúng instruction và tạo ra được những bức ảnh với instruction phức tạp. Cuối năm ngoái, Apple ra mắt MobileCLIP, một họ text-encoder model thân thiện với mobile, nhỏ hơn và latency thấp hơn. Xem thêm tại đây.&lt;/li&gt;
&lt;/ul&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://atekcostorage.blob.core.windows.net/post-image/2024/6/21/1718953622585/1718954265459_mobileclip.png&quot; width=&quot;100%&quot; /&gt;&lt;br /&gt;
  &lt;i&gt;Các loại camera view cơ bản&lt;/i&gt;
&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;Về phía Image decoder, không rõ Apple hiện sử dụng model gì. Ta chỉ có thể biết Apple sử dụng một diffusion model mà thôi. Rất có thể, đó là một version sử dụng low-bit palletization (Mixed-bit palettization), tương tự như model tại đây.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;nén-model&quot;&gt;Nén model&lt;/h1&gt;

&lt;p&gt;Hiện nay, các model chạy ở local ngoài việc có số lượng parameter không quá lớn, thì thường được dùng kèm các biện pháp nén (compression). Điều này có thể giảm kích thước của model vài lần, giúp giảm bộ nhớ sử dụng một cách rất đáng kể. Hai phương pháp chính mà Apple dùng gồm có low-bit pallettization và quantization.&lt;/p&gt;

&lt;h2 id=&quot;low-bit-paletization&quot;&gt;Low-bit paletization&lt;/h2&gt;

&lt;p&gt;Đây là một phương pháp nén model của Apple, có thể xem thêm tại Palettization Overview. Giải thích đơn giản, đây là phương pháp dựa trên weight clustering, ta tạo các cluster dựa trên weight của model, sau đó tạo một bảng lookup table (LUT) tương ứng với các centroid của cluster, và lưu các weight tương ứng với index của cluster mà weight thuộc về.&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://atekcostorage.blob.core.windows.net/post-image/2024/6/21/1718953622585/1718954630770_palettization-overview.png&quot; width=&quot;100%&quot; /&gt;&lt;br /&gt;
  &lt;i&gt;Minh họa về Palettization. Nguồn: Apple&lt;/i&gt;
&lt;/div&gt;

&lt;h2 id=&quot;quantization&quot;&gt;Quantization&lt;/h2&gt;

&lt;p&gt;Quantization là phương pháp khá phổ biến hiện nay để giảm memory sử dụng khi thực hiện inference các LLM. Bản thân Apple cũng có một tool để có thể tối ưu việc lựa chọn bit rate cho các phép toán là Talaria . Apple thực hiện quantization ở activation cũng như embedding layers.&lt;/p&gt;

&lt;p&gt;Có thể xem thêm về thuật toán GPTQ và QAT Apple dùng tại đây.&lt;/p&gt;

&lt;h2 id=&quot;kết-quả&quot;&gt;Kết quả&lt;/h2&gt;

&lt;p&gt;Kết hợp giữa việc sử dụng các foundation model kích thước nhỏ, các biện pháp tối ưu và nén model, Apple có thể cho ra kết quả rất đáng kinh ngạc: trên iPhone 15 Pro, time-to-first-token latency chỉ khoảng 0.6ms (trên prompt token), cũng như generation rate đạt tới 30 tokens/s. Một con số rất ổn cho nhiều tác vụ trên một thiết bị vô cùng nhỏ gọn.&lt;/p&gt;

&lt;h1 id=&quot;sử-dụng-các-adapter-lora&quot;&gt;Sử dụng các Adapter (LoRA)&lt;/h1&gt;

&lt;p&gt;Để có thể thực hiện các tác vụ chuyên môn hóa tốt hơn, đặc biệt khi sử dụng các foundation với kích thước nhỏ và đã được nén nhiều lần, Apple sử dụng rất nhiều bộ Adapter cho nhiệm vụ này.&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://atekcostorage.blob.core.windows.net/post-image/2024/6/21/1718953622585/1718954262951_adapters.png&quot; width=&quot;100%&quot; /&gt;&lt;br /&gt;
  &lt;i&gt;Nhiều Adapter ứng với nhiều tác vụ. Nguồn: Apple&lt;/i&gt;
&lt;/div&gt;

&lt;p&gt;Các Adapter, về cơ bản chính là các module neural network nhỏ, có thể ghép vào các layer của một model, sau đó được fine-tune cho các tác vụ chuyên biệt. Với việc sử dụng Adapters, foundation model vẫn được giữ nguyên, đảm bảo về memory cũng như các kiến thức chung, trong khi vẫn có thể cung cấp giải pháp chuyên môn hóa tối ưu cho từng tác vụ.&lt;/p&gt;

&lt;p&gt;Các Adapters cũng vì thế mà có kích thước vừa phải, chỉ khoảng vài chục MB. Kích thước nhỏ của các Adapters tạo điều kiện cho các thiết kế UX và workflow linh hoạt hơn.&lt;/p&gt;

&lt;p&gt;Để tìm hiểu thêm về Adapter, bạn có thể xem thêm về LoRA (Low-Rank Adaptation) và các biến thể khác.&lt;/p&gt;

&lt;h1 id=&quot;agentic-workflow&quot;&gt;Agentic workflow&lt;/h1&gt;

&lt;p&gt;Trong keynote của mình, Apple đã sử dụng rất nhiều ví dụ để cho thấy sự linh hoạt của Apple Intelligence với các input, ví dụ như:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Show me all the photo of Mom, Olivia and me&lt;/li&gt;
  &lt;li&gt;Pull up the files that Joz shared with me last week&lt;/li&gt;
  &lt;li&gt;Play the podcast that my wife sent the other day&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Với việc sử dụng nhiều foundation model và các adapter để điều chỉnh, Apple Intelligence không chỉ hoạt động như một model đơn lẻ, một công cụ hỗ trợ cho một tác vụ cụ thể mà cần phải linh hoạt trong các tác vụ, có thể tự động lựa chọn, điều phối các model liên quan dựa trên các yêu cầu của người dùng.&lt;/p&gt;

&lt;p&gt;Để làm được điều này, nhiều khả năng Apple đã sử dụng một hệ thống agentic để điều phối dựa trên context và yêu cầu của người dùng. Để hiểu thêm về agentic, bạn có thể xem thêm bài viết Agentic Workflow: Hiệu quả hơn nhờ ‘mô phỏng’ chuyên gia.&lt;/p&gt;

&lt;p&gt;Ví dụ như Apple sử dụng nhiều adapter (LoRA) cho các style image generation khác nhau, thì khi đó, việc chọn Adapter nào là phù hợp để dùng là một tác vụ cần agentic. Bạn có thể xem thêm một agentic workflow tương tự là Stylus.&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://atekcostorage.blob.core.windows.net/post-image/2024/6/21/1718953622585/1718954268382_stylus-gif-final.gif&quot; width=&quot;100%&quot; /&gt;&lt;br /&gt;
&lt;/div&gt;

&lt;h1 id=&quot;một-điều-nữa&quot;&gt;Một điều nữa&lt;/h1&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://atekcostorage.blob.core.windows.net/post-image/2024/6/21/1718953622585/1718954267668_siri-semantic-search.png&quot; width=&quot;100%&quot; /&gt;&lt;br /&gt;
&lt;/div&gt;

&lt;p&gt;Tuy không được nhắc tới, nhưng với tính năng Semantic search của Siri, có lẽ Apple đã sử dụng RAG để thực hiện tính năng này. Để có thể tìm kiếm dựa trên ý nghĩa thay vì keyword, nhiều khả năng Apple thực hiện:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Một vector database xây dựng sẵn trong thiết bị, và index là multimodal (text, images, record)&lt;/li&gt;
  &lt;li&gt;Nhiều khả năng Apple cũng có kèm theo một re-ranking model để cho ra kết quả tốt hơn, điều thường gặp ở Advanced RAG&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;vài-suy-nghĩ-cuối&quot;&gt;Vài suy nghĩ cuối&lt;/h1&gt;

&lt;p&gt;Một trong những vấn đề lớn của GenAI là việc nó cần nguồn tài nguyên khổng lồ để có thể hoạt động. Tuy nhiên, Apple đã chứng minh rằng việc đưa các mô hình GenAI lên thiết bị di động không chỉ khả thi mà còn có thể đạt được hiệu suất và chất lượng cao.&lt;/p&gt;

&lt;p&gt;Từ những điều mà Apple đã làm, chúng ta có thể học hỏi được rất nhiều kỹ thuật trong việc ứng dụng các mô hình GenAI trong các ứng dụng, và phần lớn những phương pháp này có thể ứng dụng rộng khắp, không chỉ riêng trên các thiết bị của Apple mà ở bất cứ đâu.&lt;/p&gt;

&lt;h1 id=&quot;tham-khảo&quot;&gt;Tham khảo&lt;/h1&gt;

&lt;p&gt;WWDC24: Platforms State of the Union&lt;/p&gt;

&lt;p&gt;From WWDC 2024, Apple Intelligence to the race of Open-Source model: Uncovering the Story Behind Apple’s Lightweight On-Device GenAI Language Model&lt;/p&gt;

&lt;p&gt;Introducing Apple’s On-Device and Server Foundation Models - Apple Machine Learning Research&lt;/p&gt;

&lt;p&gt;Talking Tech and AI with Tim Cook!&lt;/p&gt;

&lt;p&gt;Understanding Apple’s On-Device and Server Foundation Models release&lt;/p&gt;

&lt;p&gt;Use Core ML Tools for machine learning model compression&lt;/p&gt;

</description>
        <pubDate>Mon, 24 Jun 2024 00:00:00 +0700</pubDate>
        <link>http://localhost:4000/How-Apple-Intelligence-work/</link>
        <guid isPermaLink="true">http://localhost:4000/How-Apple-Intelligence-work/</guid>
        
        
      </item>
    
      <item>
        <title>Vì sao AI tạo ảnh đã giỏi đánh vần hơn xưa?</title>
        <description>&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://atekcostorage.blob.core.windows.net/post-image/2024/3/28/1711593826948/1711619199692_genai_(3).jpg&quot; width=&quot;100%&quot; /&gt;&lt;br /&gt;
&lt;/div&gt;

&lt;p&gt;Cộng đồng công nghệ những năm gần đây chứng kiến sự phát triển vượt trội và vô cùng nhanh chóng của các mô hình tạo ảnh. Từ những tấm ảnh rất dễ bị lỗi, không quá phức tạp với độ phân giải thấp, giờ đây các mô hình đã có thể tạo ra các hình ảnh tốt hơn, phức tạp hơn với độ phân giải tốt, phục vụ cho nhiều ứng dụng.&lt;/p&gt;

&lt;p&gt;Một trong những điểm được cải thiện rất nhiều trong những năm vừa qua là hình ảnh do AI tạo ra dần dần ít sai lỗi chính tả hơn. Chính xác là các model đã có khả năng text rendering (hiển thị chữ trong ảnh) tốt hơn. Những cải thiện này đến từ sự thay đổi của cơ chế tokenizer và text encoder, sẽ được giải thích cụ thể trong bài viết này.&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://atekcostorage.blob.core.windows.net/post-image/2024/3/28/1711593826948/1711594019618_image-90.png&quot; width=&quot;100%&quot; /&gt;&lt;br /&gt;
  &lt;i&gt;Prompt: Epic anime artwork of a wizard atop a mountain at night casting a cosmic spell into the dark sky that says &quot;Stable Diffusion 3&quot; made out of colorful energy - SD3&lt;/i&gt;
&lt;/div&gt;

&lt;h1 id=&quot;1-cơ-chế-tokenizer-và-năng-lực-đánh-vần&quot;&gt;1. Cơ chế tokenizer và năng lực đánh vần&lt;/h1&gt;

&lt;p&gt;Cũng như cái tên text-to-image, các model hiện nay có thể chia làm hai phần khác nhau: phần xử lý text và phần tạo ảnh.&lt;/p&gt;

&lt;p&gt;Trước hết, chúng ta cần biết cách xử lý text của các model hiện nay là đều thông qua một hệ thống tokenization riêng. Tokenizer có nhiều cách chuyển đổi văn bản đầu vào thành dữ diệu số để model có thể xử lý, có thể theo word level, character level hoặc sub-word level. Trong đó, sub-word level hiện là phương pháp hiệu quả nhất và tất cả các model phổ biến đều áp dụng.&lt;/p&gt;

&lt;p&gt;Nói một cách đơn giản, các model không xử lý văn bản đầu vào dựa theo các từ (vd: elephants) hay ký tự (vd: e, l, e, p, h, a, n, t, s) mà chia nhỏ các từ theo tổ hợp như ví dụ đơn giản ở hình dưới. Hãy tưởng tượng tới cách chúng ta hay nhìn đuôi “ion” là danh từ, “er” để chỉ người, về cơ bản chính là như vậy.&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://atekcostorage.blob.core.windows.net/post-image/2024/3/28/1711593826948/1711594017436_pasted_image_20240325144344.png&quot; width=&quot;100%&quot; /&gt;&lt;br /&gt;
  &lt;i&gt;Ví dụ về tokenizer&lt;/i&gt;
&lt;/div&gt;

&lt;p&gt;Do sub-word level tokenizer không chia theo các ký tự, việc “đánh vần” các ký tự để hiển thị trong ảnh là một điều không đơn giản với các model này. Và khi input đã không chính xác, thì làm sao output có thể chính xác được?&lt;/p&gt;

&lt;p&gt;Nói thêm về vấn đề này, chúng ta có thể bất ngờ trước việc các LLMs thực hiện các phép + - rất tệ, đó là bởi vì các LLMs cũng không được “nhìn” thấy các chữ số. Trường hợp này có thể liên tưởng tới việc chúng ta thực hiện phép nhân 2 số hệ thập lục phân: 1A * B7, sẽ phức tạp hơn nhiều lần dù bản chất chỉ là phép nhân 26 * 183.&lt;/p&gt;

&lt;p&gt;Tuy rằng các model gặp nhiều khó khăn trong việc “đánh vần” nhưng khi chúng ta tăng size của model, khả năng “đánh vần” sẽ được cải thiện rất nhiều. Với những model nhỏ như CLIP (chỉ có vài chục - vài trăm triệu parameters), khả năng “đánh vần” đương nhiên là rất kém, và điều này được cải thiện với các model CLIP size lớn hơn, tới các model vài tỷ parameters như T5 thì khá tốt. Lẽ dĩ nhiên, những model lớn như GPT-4 thì chắc không cần phải kiểm tra.&lt;/p&gt;

&lt;h1 id=&quot;2-sự-thay-đổi-trong-text-encoder-của-các-model&quot;&gt;2. Sự thay đổi trong text encoder của các model&lt;/h1&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://atekcostorage.blob.core.windows.net/post-image/2024/3/28/1711593826948/1711594018297_pasted_image_20240326094951.png&quot; width=&quot;100%&quot; /&gt;&lt;br /&gt;
&lt;/div&gt;

&lt;p&gt;Qua thời gian, các text encoder cũng dần to hơn, và đi cùng là khả năng text rendering được cải thiện rõ rệt.&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://atekcostorage.blob.core.windows.net/post-image/2024/3/28/1711593826948/1711594015059_pasted_image_20240325133620.png&quot; width=&quot;100%&quot; /&gt;&lt;br /&gt;
  &lt;i&gt;Prompt: Sprouts in the shape of text &apos;Imagen&apos; coming out of a fairytale book - Imagen
&lt;/i&gt;
&lt;/div&gt;

&lt;p&gt;Vậy tại sao những model ban đầu lại sử dụng CLIP? Bởi vì những model ban đầu có kích thước rất nhỏ, và có lẽ khi đó text rendering cũng không phải là vấn đề cần chú ý. Ví dụ như đối với SD 1.5 chỉ có 860M params tổng cộng, thì rõ ràng dùng 123M param text encoder sẽ hợp lý hơn dùng những text encoder 695M hay vài tỷ param.&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;https://atekcostorage.blob.core.windows.net/post-image/2024/3/28/1711593826948/1711594018871_screenshot_2024-03-26_095926.png&quot; width=&quot;100%&quot; /&gt;&lt;br /&gt;
&lt;/div&gt;

&lt;p&gt;Như vậy, sự cải thiện về mặt text rendering trong các model hình ảnh hiện nay chủ yếu bắt nguồn từ việc các model hiện tại đã sử dụng các model text encoder tốt hơn, có khả năng “đánh vần” tốt hơn, từ đó tạo ra các hình ảnh chính xác hơn về mặt các ký tự.&lt;/p&gt;

&lt;p&gt;Còn về phía người dùng, có lẽ chúng ta chỉ cần đọc các bài đánh giá trên mạng, xem một vài ảnh mẫu và dùng thử là có cảm nhận riêng, từ đó lựa chọn model phù hợp với nhu cầu cá nhân để sử dụng.&lt;/p&gt;

&lt;h1 id=&quot;tham-khảo&quot;&gt;Tham khảo&lt;/h1&gt;

&lt;p&gt;TextDiffuser: Diffusion Models as Text Painters (arxiv.org)&lt;/p&gt;

&lt;p&gt;SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis (arxiv.org)&lt;/p&gt;

&lt;p&gt;Stable Diffusion 3: Research Paper — Stability AI&lt;/p&gt;

&lt;p&gt;EVA-CLIP: Improved Training Techniques for CLIP at Scale (arxiv.org)&lt;/p&gt;

</description>
        <pubDate>Thu, 28 Mar 2024 00:00:00 +0700</pubDate>
        <link>http://localhost:4000/why-image-gen-could-spell-now/</link>
        <guid isPermaLink="true">http://localhost:4000/why-image-gen-could-spell-now/</guid>
        
        
      </item>
    
      <item>
        <title>My tech recap - 2023</title>
        <description>&lt;p&gt;Another new year is here. 2023 was all about GenAI, from its tech buzz to the drama. I took a U-turn, diving deep into AI once again. I want to recap what I learned and made last year and share my thoughts and plans for this year. So, let’s begin!&lt;/p&gt;

&lt;h1 id=&quot;what-i-learned-in-2023&quot;&gt;What I Learned in 2023&lt;/h1&gt;

&lt;h2 id=&quot;genai---llms&quot;&gt;GenAI - LLMs&lt;/h2&gt;

&lt;p&gt;As an AI person, of course I have to learn about LLM this year. I involved in some projects using it, from RAG with vector DB (yeah, everyone makes this too), to make query from user input, and using LLM to do many NLP tasks.. LLM has changed the NLP game forever.&lt;/p&gt;

&lt;p&gt;What fascinated me the most was the concept of agents. I believe it will need custom tweaks for each company, but eventually, most will adopt it.&lt;/p&gt;

&lt;p&gt;I write about GenAI here: https://tulip4attoo.substack.com/&lt;/p&gt;

&lt;p&gt;Some areas I haven’t explored much:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;LLM training: It’s costly, but I think for around $10k-$50k, many companies could give it a shot.&lt;/li&gt;
  &lt;li&gt;LLM finetuning: I’ve mainly used prompt engineering instead. However, for my projects, I’d consider converting these into a finetuned model.&lt;/li&gt;
  &lt;li&gt;Agent: I’ve just started understanding how to tune a model into an agent. This seems promising.&lt;/li&gt;
  &lt;li&gt;Detailed techniques: There are various techniques to enhance results, like:&lt;/li&gt;
&lt;/ul&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;../assets/img/recap/2023-RAG-techniques.png&quot; width=&quot;100%&quot; /&gt;&lt;br /&gt;
  &lt;i&gt;&lt;/i&gt;
&lt;/div&gt;

&lt;h2 id=&quot;genai---images&quot;&gt;GenAI - Images&lt;/h2&gt;

&lt;p&gt;I’ve dabbled a bit here. The models are impressive. However, I’m not interested in making money from controversial content.&lt;/p&gt;

&lt;p&gt;My biggest regret in 2023 was failing to launch an AI avatar app (I got it in May, and you need ~1-5s to make this with only 1 photo)&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;../assets/img/recap/2023-ai-avatar.jpg&quot; width=&quot;100%&quot; /&gt;&lt;br /&gt;
  &lt;i&gt;&lt;/i&gt;
&lt;/div&gt;

&lt;h2 id=&quot;serverless-gpu&quot;&gt;Serverless GPU&lt;/h2&gt;

&lt;p&gt;In 2023, I realized that major cloud providers don’t offer Serverless GPU services. I used Modal to deploy my AI avatar app. It ran okay but was slow to wake up, even when not in a cold start. Maybe I need to improve this.&lt;/p&gt;

&lt;p&gt;I also tried Salad, but they lacked documentation and weren’t user-friendly.&lt;/p&gt;

&lt;p&gt;Dedicated GPU servers from smaller providers are much cheaper, but they require manual scaling, unlike serverless options which are easy to set up.&lt;/p&gt;

&lt;h2 id=&quot;fastapi&quot;&gt;FastAPI&lt;/h2&gt;

&lt;p&gt;I used this to create APIs. Nothing much to say here.&lt;/p&gt;

&lt;h2 id=&quot;new-coding-paradigm&quot;&gt;New Coding Paradigm&lt;/h2&gt;

&lt;p&gt;VSCode with Github Copilot and GPT-plus has changed how I code. Now, I manage a pilot to code rather than doing it myself. But I still read each line because delivering quality work is what matters most.&lt;/p&gt;

&lt;h2 id=&quot;building-things-principles&quot;&gt;Building Things Principles&lt;/h2&gt;

&lt;p&gt;Any principle could be right, but you need to choose what works for you and improves your results. For me, it’s about being fast and efficient.&lt;/p&gt;

&lt;h1 id=&quot;my-current-stackflow&quot;&gt;My current stack/flow&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;VSCode with Github Copilot:&lt;/strong&gt; I’m considering canceling my subscription and looking for alternatives. I do think Mixtral one is the best replacement, and I even could host it somewhere to use it (or local with Mistral 7B).&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;GPT-Plus for Coding and Research:&lt;/strong&gt; I haven’t found anything that compares to GPT-Plus. It’s even better than GPT-4 for certain tasks.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Gradio for Quick MVPs:&lt;/strong&gt; It’s my go-to for creating minimum viable products.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Obsidian for Logs and Notes:&lt;/strong&gt; It’s fast and easy to organize. Combined with Git, it’s incredibly efficient.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;FastAPI for Backend:&lt;/strong&gt; It’s my choice for backend development.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Plotly/Dash for Visuals:&lt;/strong&gt; Great for creating visual components.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;what-i-plan-to-learnmake-in-2024&quot;&gt;What I Plan to Learn/Make in 2024&lt;/h1&gt;

&lt;h2 id=&quot;launch-a-complete-system-and-company&quot;&gt;Launch a Complete System and Company&lt;/h2&gt;

&lt;p&gt;I’m aiming to launch a product or company, likely related to LLM, but I’m open to other areas as well.&lt;/p&gt;

&lt;h2 id=&quot;rust--bevy&quot;&gt;Rust + Bevy&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Rust:&lt;/strong&gt; I want to master Rust to develop quick and lightweight 2D prototypes. I’m not sure about smartphone development, but for game development, I’m interested in Bevy.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Bevy for Game Development:&lt;/strong&gt; I’m drawn to Bevy, especially as Unity isn’t very appealing right now. I’ll stick with Unreal Engine for 3D projects, but those are more time-consuming than I can manage at the moment.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This YouTube clip inspired me to explore Bevy:&lt;/p&gt;

&lt;div style=&quot;position:relative;padding-top:56.25%;&quot;&gt;
  &lt;iframe src=&quot;https://www.youtube.com/embed/8kLWBDbVP0I&quot; frameborder=&quot;0&quot; style=&quot;position:absolute;top:0;left:0;width:100%;height:100%;&quot; allowfullscreen=&quot;&quot;&gt;
    &lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;Hope that I could proudly write much more in the next year.&lt;/p&gt;
</description>
        <pubDate>Fri, 05 Jan 2024 00:00:00 +0700</pubDate>
        <link>http://localhost:4000/My-tech-recap-2023/</link>
        <guid isPermaLink="true">http://localhost:4000/My-tech-recap-2023/</guid>
        
        
      </item>
    
      <item>
        <title>Collision Filtering trong UE</title>
        <description>&lt;p&gt;Bài này nói về chuyện collide (va chạm) trong UE, những cái responses khác nhau với collision, cách dùng channels để filter collisions, cũng như liệt ra các điểm khác nhau giữ simple và complex collision geometry.&lt;/p&gt;

&lt;h1 id=&quot;blocking-overlapping-và-ignoring&quot;&gt;Blocking, Overlapping và Ignoring&lt;/h1&gt;

&lt;p&gt;Tương tự bên Unity, có 3 kiểu: blocking, overlapping và ignoring. Một wall có thể “block” player, nhưng trigger có thể “overlap” nó, và trigger có thể xuyên qua. 2 trường hợp đều tạo ra 1 event, nhưng mà khác biệt nhau: “hit” và “overlap”.&lt;/p&gt;

&lt;p&gt;“Ignore” là bỏ qua luôn. Không có event không có gì hết.&lt;/p&gt;

&lt;p&gt;Tức là có tổng cộng 3 kiểu responses types.&lt;/p&gt;

&lt;h1 id=&quot;trace-channels-and-object-channels&quot;&gt;&lt;strong&gt;Trace Channels and Object Channels&lt;/strong&gt;&lt;/h1&gt;

&lt;p&gt;Hãy tưởng tượng ta có 1 thiết kế như này: Player thì sẽ bị wall block, còn đi xuyên cây. Vì ta set cả Shrub và Wall là WorldStatic, nên làm cách nào để Player có thể chỉ đi xuyên qua Shrub?&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;../assets/img/collision-filtering-ue/tinh-huong.png&quot; width=&quot;100%&quot; /&gt;&lt;br /&gt;
  &lt;i&gt;Các loại camera view cơ bản&lt;/i&gt;
&lt;/div&gt;

&lt;p&gt;UE đã giải quyết nó bằng cách tạo ra 2 lớp đối chiếu: khi vật A collide với vật B, thì ta sẽ đồng thời tính toán:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Vật A sẽ response với vật B như nào (Blocking, Overlapping và Ignoring)&lt;/li&gt;
  &lt;li&gt;Vật B sẽ response với vật A như nào (Blocking, Overlapping và Ignoring)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Sau khi có 2 thông tin này, ta sẽ tổng hợp lại theo bảng sau:&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;../assets/img/collision-filtering-ue/table.png&quot; width=&quot;100%&quot; /&gt;&lt;br /&gt;
  &lt;i&gt;Bảng đối chiếu response&lt;/i&gt;
&lt;/div&gt;

&lt;p&gt;Dễ thấy Player và Shrub là Block + Overlap = Overlap, còn Player với Wall là Block + Block = Block.&lt;/p&gt;

&lt;p&gt;Do đó, kết quả là như này:&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;../assets/img/collision-filtering-ue/shrub.png&quot; width=&quot;100%&quot; /&gt;&lt;br /&gt;
  &lt;i&gt;Kết quả các event&lt;/i&gt;
&lt;/div&gt;

&lt;p&gt;Nếu trong game, ta có 2 player và muốn 1 player trở nên “ghostly”, ta chỉ việc đổi cái response to Pawn channel từ Block sang Ignore mà thôi.&lt;/p&gt;

&lt;h1 id=&quot;collision-presets&quot;&gt;&lt;strong&gt;Collision Presets&lt;/strong&gt;&lt;/h1&gt;

&lt;p&gt;Tuy nhiên setup tay cũng mệt, UE có sẵn 1 số cái preset cho nhiều built (invi wall, physic actors,…)&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;../assets/img/collision-filtering-ue/preset.png&quot; width=&quot;100%&quot; /&gt;&lt;br /&gt;
  &lt;i&gt;Collision preset&lt;/i&gt;
&lt;/div&gt;

&lt;p&gt;Nếu không muốn thì ta có thể custom&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;../assets/img/collision-filtering-ue/custom.png&quot; width=&quot;100%&quot; /&gt;&lt;br /&gt;
  &lt;i&gt;Collision custom set&lt;/i&gt;
&lt;/div&gt;

&lt;h1 id=&quot;simple-and-complex-collision&quot;&gt;&lt;strong&gt;Simple and Complex Collision&lt;/strong&gt;&lt;/h1&gt;

&lt;p&gt;Ở UE, mỗi object có thể có “complex” và “simple” collision representation. Complex Collision thì refers về việc sử dụng cái rendering geometry cho collision. Cái này thì vd như weapon traces, ta muốn bắn thì trúng tường hay các vị trí mà ta nhìn thấy (cũng ko quá nhiều cases). Simple thì đơn giản hơn và nhẹ hơn.&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;../assets/img/collision-filtering-ue/just_collision.png&quot; width=&quot;100%&quot; /&gt;&lt;br /&gt;
  &lt;i&gt;&lt;/i&gt;
&lt;/div&gt;

&lt;div align=&quot;center&quot;&gt;
  &lt;img src=&quot;../assets/img/collision-filtering-ue/collision_view.png&quot; width=&quot;100%&quot; /&gt;&lt;br /&gt;
  &lt;i&gt;Collision view&lt;/i&gt;
&lt;/div&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;

&lt;p&gt;Vậy là hết rùi đó. Cũng đơn giản dễ hiểu, và khá khá giống Unity nhỉ hehe.&lt;/p&gt;
</description>
        <pubDate>Sat, 21 May 2022 00:00:00 +0700</pubDate>
        <link>http://localhost:4000/Collision-Filtering-trong-UE/</link>
        <guid isPermaLink="true">http://localhost:4000/Collision-Filtering-trong-UE/</guid>
        
        
      </item>
    
  </channel>
</rss>
